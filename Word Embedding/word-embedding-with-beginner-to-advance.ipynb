{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "0adf4f008233f87f62901b09885e68d45cacb6ce"
   },
   "source": [
    "# **Word embedding with Python**\n",
    "**word2vec, doc2vec, GloVe implementation with Python**\n",
    "\n",
    "---\n",
    "![](https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcS7lrYJSLlPvn3Hoeo24Y2NAze3ZLMsRdxibZR1MsMCiHkwHXAS)\n",
    "![](https://s3-ap-south-1.amazonaws.com/av-blog-media/wp-content/uploads/2017/06/06062705/Word-Vectors.png)\n",
    "\n",
    "---\n",
    "\n",
    "### **Table of Contents**\n",
    "\n",
    "---\n",
    "* [**1.What are Word Embeddings?**](#1.What-are-Word-Embeddings?)\n",
    "* [**2.Different types of Word Embedding**](#2.-Different-types-of-Word-Embedding)\n",
    "    * [**2.1.Frequency based Embedding**](#2.1.Frequency-based-Embedding)  \n",
    "        * [**2.1.1.Count Vectors**](#2.1.1.Count-Vectors)  \n",
    "        * [**2.1.2.TF-IDF**](#2.1.2.TF-IDF)  \n",
    "        * [**2.1.3.Co-Occurrence Matrix**](#2.1.3.Co-Occurrence-Matrix)  \n",
    "    * [**2.2.Prediction based Embedding**](#2.2.Prediction-based-Embedding)  \n",
    "        * [**2.2.1.CBOW**](#2.2.1.CBOW)  \n",
    "        * [**2.2.2.Skip-Gram**](#2.2.2.Skip-Gram)  \n",
    "* [**3.Using pre-trained Word Vectors**](#3.Using-pre-trained-Word-Vectors)\n",
    "* [**4.Training your own Word Vectors**](#5.Training-your-own-Word-Vectors)\n",
    "\n",
    "---\n",
    "\n",
    "# ***1.What are Word Embeddings?***\n",
    "\n",
    "---\n",
    "# **Defination**\n",
    "\n",
    "> ## **Word embeddings are a type of word representation that allows words with similar meaning to have a similar representation.** ***...By Jason Brownlee.***  \n",
    "---\n",
    "### **Example 1**\n",
    "![](https://i.stack.imgur.com/oJEie.png)\n",
    "\n",
    "### **Example 2**\n",
    "![](https://cdn-images-1.medium.com/max/1600/1*YEJf9BQQh0ma1ECs6x_7yQ.png)\n",
    "\n",
    "* A very basic definition of a word embedding is a real number, vector representation of a word. Typically, these days, words with similar meaning will have vector representations that are close together in the embedding space (though this hasn’t always been the case).\n",
    "\n",
    "* ***Word embedding is a dense representation of words in the form of numeric vectors. It can be learned using a variety of language models. The word embedding representation is able to reveal many hidden relationships between words. For example, vector(“cat”) - vector(“kitten”) is similar to vector(“dog”) - vector(“puppy”). This post introduces several models for learning word embedding and how their loss functions are designed for the purpose.***\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "847128166ca8b7a314c25c71b3420ae7aead4cf7"
   },
   "source": [
    "---\n",
    "\n",
    "# ***2.Different types of Word Embedding***\n",
    "\n",
    "---\n",
    "\n",
    "### The different types of word embeddings can be broadly classified into two categories\n",
    "\n",
    "1. **Frequency based Embedding**\n",
    "1. **Prediction based Embedding**\n",
    "\n",
    "![](https://multithreaded.stitchfix.com/assets/posts/2017-10-18-stop-using-word2vec/fig_006.png)\n",
    "\n",
    "---\n",
    "## ***2.1.Frequency based Embedding***\n",
    "\n",
    "### **2.1.1.Count Vectors**\n",
    "\n",
    "---\n",
    "\n",
    "* Extract the corpus C {d1, d2 ... dD} of the document D and the N unique tokens (words) from the corpus C. N unque form our dictionary and the size of the count vector matrix M by DX N. D (i) is the number of times each row of the matrix contains M tokens in the document.\n",
    "\n",
    "#### Let us understand this with a simple example.\n",
    "\n",
    "* **D1: He is a lazy boy. She is also lazy.**\n",
    "* **D2: Neeraj is a lazy person.**\n",
    "\n",
    "The dictionary created can be a word with a **unique tag in the corpus**: ***['He', 'She', 'lazy', 'boy', 'Neeraj', 'person']***  \n",
    "* Here, **D = 2, N = 6**, The count matrix M of size 2 X 6 will be represented as –\n",
    "\n",
    "||He|She|lazy|boy|Neeraj|person|\n",
    "|--|--|--|--|--|--|--|\n",
    "|D1|1|1|2|1|0|0|\n",
    "|D2|0|0|1|0|1|1|\n",
    "\n",
    "\n",
    "### **Practical Example**\n",
    "\n",
    "### **1. Count Vectorization**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_uuid": "4c0526b2315e40a8df8fb0898040662b64c82230"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_uuid": "3c3944ff129083d6d4fd8a6a60aab923f36c419d"
   },
   "outputs": [],
   "source": [
    "text = ['The quick brown fox jumped over the lazy dog']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_uuid": "1b889013c041b2108d2bbc7298afe7612dcfc62d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'the': 7, 'quick': 6, 'brown': 0, 'fox': 2, 'jumped': 3, 'over': 5, 'lazy': 4, 'dog': 1}\n",
      "(1, 8)\n",
      "<class 'scipy.sparse.csr.csr_matrix'>\n",
      "[[1 1 1 1 1 1 1 2]]\n"
     ]
    }
   ],
   "source": [
    "vectorizer = CountVectorizer()\n",
    "vectorizer.fit(text)\n",
    "print(vectorizer.vocabulary_)\n",
    "#encode the document\n",
    "vector = vectorizer.transform(text)\n",
    "print(vector.shape)\n",
    "print(type(vector))\n",
    "print(vector.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_uuid": "6a4aa57e48231eb67526875705243ffef9cbdae4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 8)\n",
      "<class 'scipy.sparse.csr.csr_matrix'>\n",
      "[[1 1 1 1 1 1 1 2]]\n"
     ]
    }
   ],
   "source": [
    "vector = vectorizer.transform(text)\n",
    "print(vector.shape)\n",
    "print(type(vector))\n",
    "print(vector.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_uuid": "5e5c2bd216a04ed8c927c945fc416a9f25a44cba"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         0    1    2    3    4    5    6    7\n",
       "count  1.0  1.0  1.0  1.0  1.0  1.0  1.0  1.0\n",
       "mean   1.0  1.0  1.0  1.0  1.0  1.0  1.0  2.0\n",
       "std    NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN\n",
       "min    1.0  1.0  1.0  1.0  1.0  1.0  1.0  2.0\n",
       "25%    1.0  1.0  1.0  1.0  1.0  1.0  1.0  2.0\n",
       "50%    1.0  1.0  1.0  1.0  1.0  1.0  1.0  2.0\n",
       "75%    1.0  1.0  1.0  1.0  1.0  1.0  1.0  2.0\n",
       "max    1.0  1.0  1.0  1.0  1.0  1.0  1.0  2.0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector.toarray()\n",
    "df = pd.DataFrame(vector.todense())\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "675d4547991c985548fa82507e9c698c10845908"
   },
   "source": [
    "## **2.1.2.TF-IDF**\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "![](https://lh4.googleusercontent.com/zSHLtG-IVPQG_raN67XTAuzIlKpJs8dkZtFP3VhN7W8Ur4keIzgRt8_w1eyqQ8lyX1flOqyf4xhrOoXUzLRHfzgCQhurjouJFyQaPMahHb8Ar5TH5L96T8QTQGKF7C90wvYhjvPOshzbyK4zSA)\n",
    "\n",
    "### **Formula**\n",
    "![](https://cdn-images-1.medium.com/max/1600/1*jNnpbGPxkjehlvTCXq9B8g.png)\n",
    "1.  **TF Score (Term Frequency)** :\n",
    "Considers documents as bag of words, agnostic to order of words. A document with 10 occurrences of the term is more relevant than a document with term frequency 1. But it is not 10 times more relevant, relevance is not proportional to frequency\n",
    "\n",
    "2.  **IDF Score (Inverse Document Frequency)**\n",
    "We also want to use the frequency of the term in the collection for weighting and ranking. Rare terms are more informative than frequent terms. We want low positive weights for frequent terms and high weights for rare terms.\n",
    "\n",
    "### **Mathematical Example**\n",
    "\n",
    "![image.png](https://s3-ap-south-1.amazonaws.com/av-blog-media/wp-content/uploads/2017/06/04171138/Tf-IDF.png)\n",
    "\n",
    " \n",
    "### ***Term Frequency***\n",
    "\n",
    "---\n",
    "TF(***Term Frequency***) = (**Number of times term t appears in a document)/(Number of terms in the document)**\n",
    "\n",
    "* **TF(This,Document1)** = 1/8\n",
    "* **TF(This, Document2)**=1/5\n",
    "\n",
    "It **denotes the contribution of the word to the document i.e words relevant to the document should be frequent.** eg: **A document about Messi should contain the word ‘Messi’ in large number.**\n",
    "\n",
    "### ***Inverse Document Frequency***\n",
    "\n",
    "---\n",
    "IDF(***Inverse Document Frequency***) = **log(N/n)**, where, **N is the number of documents and n is the number of documents a term t has appeared in.**\n",
    "\n",
    "* where **N is the number of documents** and **n is the number of documents a term t has appeared in.**\n",
    "\n",
    "* **IDF(This) = log(2/2) = 0.**\n",
    "\n",
    "* So, how do we explain the reasoning behind IDF? Ideally, if a word has appeared in all the document, then probably that word is not relevant to a particular document. But if it has appeared in a subset of documents then probably the word is of some relevance to the documents it is present in.\n",
    "\n",
    "Let us compute IDF for the word ‘Messi’.\n",
    "\n",
    "* **IDF(Messi)** = log(2/1) = 0.301.\n",
    "\n",
    "Now, let us compare the TF-IDF for a common word ‘This’ and a word ‘Messi’ which seems to be of relevance to Document 1.\n",
    "\n",
    "* TF-IDF(This,Document1) = (1/8) * (0) = 0\n",
    "* TF-IDF(This, Document2) = (1/5) * (0) = 0\n",
    "* TF-IDF(Messi, Document1) = (4/8)*0.301 = 0.15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "_uuid": "0863e08f3b2f4cb19af10e7701d7f12fe4507157"
   },
   "outputs": [],
   "source": [
    "text_2 = ['The quick brown fox jumped over the lazy dog','The dog','The fox']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "_uuid": "ce1f77c5f191d26504d23c00eb7c6daedb16ca12"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'the': 7, 'quick': 6, 'brown': 0, 'fox': 2, 'jumped': 3, 'over': 5, 'lazy': 4, 'dog': 1}\n",
      "[1.69314718 1.28768207 1.28768207 1.69314718 1.69314718 1.69314718\n",
      " 1.69314718 1.        ]\n",
      "(3, 8)\n",
      "[[0.36388646 0.27674503 0.27674503 0.36388646 0.36388646 0.36388646\n",
      "  0.36388646 0.42983441]\n",
      " [0.         0.78980693 0.         0.         0.         0.\n",
      "  0.         0.61335554]\n",
      " [0.         0.         0.78980693 0.         0.         0.\n",
      "  0.         0.61335554]]\n"
     ]
    }
   ],
   "source": [
    "#create the transform\n",
    "vectorizer_2 = TfidfVectorizer()\n",
    "#tokenize and build vocab\n",
    "vectorizer_2.fit(text_2)\n",
    "# summarize\n",
    "print(vectorizer_2.vocabulary_)\n",
    "print(vectorizer_2.idf_)\n",
    "#encode document\n",
    "vector_2 = vectorizer_2.transform(text_2)\n",
    "#summarize encode vector\n",
    "print(vector_2.shape)\n",
    "print(vector_2.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "_uuid": "7c6778ce49cdb2b73467fc29f3a667092bf1ded3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 8)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector_2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "_uuid": "48eeb9f4958a0379f74ff1049e8c33a81b870aa0"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.363886</td>\n",
       "      <td>0.276745</td>\n",
       "      <td>0.276745</td>\n",
       "      <td>0.363886</td>\n",
       "      <td>0.363886</td>\n",
       "      <td>0.363886</td>\n",
       "      <td>0.363886</td>\n",
       "      <td>0.429834</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.789807</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.613356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.789807</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.613356</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1         2    ...            5         6         7\n",
       "0  0.363886  0.276745  0.276745    ...     0.363886  0.363886  0.429834\n",
       "1  0.000000  0.789807  0.000000    ...     0.000000  0.000000  0.613356\n",
       "2  0.000000  0.000000  0.789807    ...     0.000000  0.000000  0.613356\n",
       "\n",
       "[3 rows x 8 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector_2.toarray()\n",
    "pd.DataFrame(vector_2.todense())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "_uuid": "51149b8988df9131e03e01e8937a321fb66d576f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method DataFrame.info of    0  1  2  3  4  5  6  7\n",
       "0  1  1  1  1  1  1  1  2>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "_uuid": "79686287ba05b351b85dee870ee2b8046f2a4a23"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.9746629 , 0.60800305, 0.60800305]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# trying to get cosine similarity\n",
    "cosine_similarity(vector,vector_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "14d79caa34c3b8c90e40f25d365a9d771c37da97"
   },
   "source": [
    "## **2.1.3.Co-Occurrence Matrix**\n",
    "\n",
    "---\n",
    "\n",
    "![](https://slideplayer.com/slide/9474494/29/images/12/Computation+of+Co-occurrence+Matrix.jpg)\n",
    "\n",
    "**The big idea:** similar words tend to happen together and will have a similar context, for example, Apple is a fruit. Mango is a fruit.Apples and mangos tend to have a similar background, namely fruits. Before delving deeper into the details of **constructing a co-occurrence matrix,** two concepts need to be clarified: **co-occurrence and context limitations.**\n",
    "\n",
    "* **Co-occurrence:** for a given corpus, **_the symbiosis of a pair of words w1 and w2 is the number of times they appear together within the context boundary._**\n",
    "* **Context limits:** Context limits are specified by **_numbers and addresses. So, what does the context limit 2 (around) mean? Let's see an example._**\n",
    "![](https://frproxy.vpnbook.com/browse.php?u=BZBC9ReUZHRUXH%2FpbtilyDmkvQv8Tx97on0Ltbk73UxyXedka8TMZfMr6kirZ3DKYs00fBVTNqjj1%2B90dw%3D%3D&b=0)\n",
    "\n",
    "The **green word is the context boundary 2 (surrounding) of the word \"Fox\"** and only these words are calculated to calculate the co-occurrence. Let's look at the context limit of the word \"Over\".\n",
    "![](https://frproxy.vpnbook.com/browse.php?u=BZBC9ReUZHRUXH%2FpbtilyDmkvQv8Tx97on0Ltbk73UxyXedka8TMZfNy%2B06mZXDKYs07fBUPNqjj1%2B90dw%3D%3D&b=0)\n",
    "\n",
    "Let's take an example to calculate a **co-occurrence matrix.**\n",
    "* **Corpus = He is not lazy. He is intelligent. He is intelligent.**\n",
    "\n",
    "![](https://frproxy.vpnbook.com/browse.php?u=BZBC9ReUY3RUXH%2FpbtilyDmkvQv8Tx97on0Ltbk73UxyXedka8TMZfx5vErhK3DKYs08fBIYcre51%2B90dw%3D%3D&b=0)\n",
    "\n",
    "Let us understand this co-occurrence matrix by using the two examples from the previous table. **Red and blue boxes.**\n",
    "\n",
    "***Red box:*** The number of occurrences of **\"He\"** and **\"East\"** within context 2, you can see that this number is 4.\n",
    "\n",
    "![](https://frproxy.vpnbook.com/browse.php?u=BZBC9ReUYnRUXH%2FpbtilyDmkvQv8Tx97on0Ltbk73UxyXedka8TMZfxluFCiJXDKYsIpfBFTJqCw1%2B90dw%3D%3D&b=0)\n",
    "\n",
    "* The word **\"Lazy\"** has never been **\"intelligent\"** in the ***context of the boundary, so it has been assigned a value of 0 in the blue box.***\n",
    "\n",
    "### **Change of co-occurrence matrix.**\n",
    "\n",
    "* Suppose there are *V unique words in the corpus. Therefore, the size of the vocabulary = V.* The columns of the ***concurrency matrix form a context word. The varied changes in the co-occurrence matrix are:*\n",
    "    1. ***V X V size co-occurrence matrix Now, a regular V body becomes very large, which will be difficult to handle.*** In general, this framework is not the first application in practice.\n",
    "    2. A ***co-occurrence matrix of size V x N, where N is a subset of V and can be obtained***, for example, by removing irrelevant words, such as invalid words, which are still very large and present. Computational difficulties\n",
    "\n",
    "However, keep in mind that this co-occurrence matrix is ​​not generally used for the vector representation of words, but is **divided into factors that use techniques such as PCA, SVD, etc. These factors form a representation of the word vector.**\n",
    "\n",
    "***For example, perform a PCA in a full-size VXV array. You will get the main components of V. You can select k components of these V. V X components.***\n",
    "\n",
    "In addition, **a word will be represented in k-dimensional form instead of v-dimensional while rigorously capturing identical semantic information. K is generally of the order of several hundred.**\n",
    "\n",
    "Next, ***PCA will do this to divide the co-occurrence matrix into three matrices U, S, and V, where U and V are orthogonal matrices. What is important is that the scalar product of U and S gives a representation of the word vector, and V gives a representation of the word context.***\n",
    "\n",
    "![](https://s3-ap-south-1.amazonaws.com/av-blog-media/wp-content/uploads/2017/06/04224842/svd2.png)\n",
    "\n",
    "**Advantages of Co-occurrence Matrix**:\n",
    "\n",
    "* It preserves the semantic relationship between words. i.e man and woman tend to be closer than man and apple.\n",
    "* It uses SVD at its core, which produces more accurate word vector representations than existing methods.\n",
    "* It uses factorization which is a well-defined problem and can be efficiently solved.\n",
    "* It has to be computed once and can be used anytime once computed. In this sense, it is faster in comparison to others.\n",
    " \n",
    "**Disadvantages of Co-Occurrence Matrix**\n",
    "\n",
    "* It requires huge memory to store the co-occurrence matrix.\n",
    "* But, this problem can be circumvented by factorizing the matrix out of the system for example in Hadoop clusters etc. and can be saved.\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "9e33e502d6e9990a992afcfe392b568b9009ed30"
   },
   "source": [
    "### **Practical Example**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "_uuid": "4e59229b7eef70f232ac8e5ece3d254c66548c0f"
   },
   "outputs": [],
   "source": [
    "# libraries we'll need\n",
    "# https://www.kaggle.com/rtatman/co-occurrence-matrix-plot-in-python\n",
    "import pandas as pd # dataframes\n",
    "from io import StringIO # string to data frame\n",
    "import seaborn as sns # plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "_uuid": "000ae60e33635977f497861b1a9d4174bcb519e7"
   },
   "outputs": [],
   "source": [
    "# read in our data & convert to a data frame\n",
    "data_tsv = StringIO(\"\"\"city    province    position\n",
    "0   Massena     NY  jr\n",
    "1   Maysville   KY  pm\n",
    "2   Massena     NY  m\n",
    "3   Athens      OH  jr\n",
    "4   Hamilton    OH  sr\n",
    "5   Englewood   OH  jr\n",
    "6   Saluda      SC  sr\n",
    "7   Batesburg   SC  pm\n",
    "8   Paragould   AR  m\"\"\")\n",
    "\n",
    "my_data_frame = pd.read_csv(data_tsv, delimiter=r\"\\s+\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "_uuid": "c71a008b58fe21228c3a7bf2eb2cc663dd9bb5ce"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>city</th>\n",
       "      <th>province</th>\n",
       "      <th>position</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Massena</td>\n",
       "      <td>NY</td>\n",
       "      <td>jr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Maysville</td>\n",
       "      <td>KY</td>\n",
       "      <td>pm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Massena</td>\n",
       "      <td>NY</td>\n",
       "      <td>m</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Athens</td>\n",
       "      <td>OH</td>\n",
       "      <td>jr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Hamilton</td>\n",
       "      <td>OH</td>\n",
       "      <td>sr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Englewood</td>\n",
       "      <td>OH</td>\n",
       "      <td>jr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Saluda</td>\n",
       "      <td>SC</td>\n",
       "      <td>sr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Batesburg</td>\n",
       "      <td>SC</td>\n",
       "      <td>pm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Paragould</td>\n",
       "      <td>AR</td>\n",
       "      <td>m</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        city province position\n",
       "0    Massena       NY       jr\n",
       "1  Maysville       KY       pm\n",
       "2    Massena       NY        m\n",
       "3     Athens       OH       jr\n",
       "4   Hamilton       OH       sr\n",
       "5  Englewood       OH       jr\n",
       "6     Saluda       SC       sr\n",
       "7  Batesburg       SC       pm\n",
       "8  Paragould       AR        m"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_data_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "_uuid": "858ec42710ba4d4b6d6b415a8ebb66e3992ee869"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>position</th>\n",
       "      <th>jr</th>\n",
       "      <th>m</th>\n",
       "      <th>pm</th>\n",
       "      <th>sr</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>province</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>AR</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>KY</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NY</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>OH</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SC</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "position  jr  m  pm  sr\n",
       "province               \n",
       "AR         0  1   0   0\n",
       "KY         0  0   1   0\n",
       "NY         1  1   0   0\n",
       "OH         2  0   0   1\n",
       "SC         0  0   1   1"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# conver to co-occurance matrix\n",
    "co_mat = pd.crosstab(my_data_frame.province, my_data_frame.position)\n",
    "co_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "_uuid": "73c8acc011f446804a6578be6a8ad93d37b4c1b6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f67167e35f8>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot heat map of co-occuance matrix\n",
    "sns.heatmap(co_mat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "703cfbc88416e0ecdbe46d665a3ae8f93ad459a9"
   },
   "source": [
    "# *2.2.Prediction based Embedding*\n",
    "\n",
    "---\n",
    "* **Word2vec** is not a single algorithm, but a **combination of two technologies - CBOW (continuous word bag) and Skip-gram model.** Both of these are shallow neural networks, which also map words to a target variable. Both techniques learn the weights represented by word vectors. \n",
    "\n",
    "## **2.2.1.CBOW**\n",
    "#### (Continuous Bag of words)\n",
    "\n",
    "* **CBOW model and the skip-gram model are based on the Huffman tree. **\n",
    "\n",
    "### **Huffman Tree**\n",
    "\n",
    "* **Huffman Tree** is a ***lossless data encoding algorithm.*** *The process behind its scheme includes sorting numerical values from a set in order of their frequency. The **least frequent numbers are gradually eliminated via the Huffman tree**, which **adds the two lowest frequencies from the sorted list in every new “branch.”** The sum is then positioned above the **two eliminated lower frequency values, and replaces them in the new sorted list**. Each time a **new branch is created**, it moves the **general direction of the tree** either to **the right (for higher values) or the left (for lower values)** When the sorted list is exhausted and the tree is complete, ***the final value is zero if the tree ended on a left number, or it is one if it ended on the right.*** This is a method of reducing complex code into simpler sequences and is common in video encoding.\n",
    "\n",
    "![](https://upload.wikimedia.org/wikipedia/commons/a/ac/Huffman_huff_demo.gif)\n",
    "\n",
    "## ***CBOW Continues...***\n",
    "\n",
    "![](https://cdn-images-1.medium.com/max/800/1*TkKW5uED9cm5xv-3JeSaEA.png)\n",
    "\n",
    "## **Forward propagation**\n",
    "* Next we look at CBOW neural network, the neural network model CBOW neural network model skip-gram is a mirror image of \n",
    "\n",
    "![](https://img-blog.csdn.net/20171205202107851?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMDY2NTIxNg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)\n",
    "\n",
    "* the figure above, the **input and output** of the input-output model **skip-gram model** is opposite to so,Here the input layer is the **input context encoded by one-hot ${x1,...,xC}$ composition**, where the window size is $C$ and the vocabulary size is $V$. The hidden layer is an N-dimensional vector. The final output layer is the output word that is also encoded by one-hot.$y$. Input vector encoded by one-hot through one $V×N$ Dimension weight matrix $W$ Connect to the hidden layer; hide the layer through one $N×V$ Weight matrix $W{'}$ Connect to the output layer. \n",
    "\n",
    "> The first step is to calculate the hidden layer.Output. as follows\n",
    "\n",
    "$h = \\frac{1}{C}W\\cdot (\\sum_{i=1}^C x_i)\\tag{$1$}$\n",
    "\n",
    "* This output is the **weighted average of the input vectors.** The hidden layer here is significantly different from the hidden layer of the ***skip-gram.***\n",
    "\n",
    "* The second part is to calculate the ***input at each node of the output layer. as follows:***\n",
    "\n",
    "$u_{j}=v^{'T}_{wj}\\cdot h\\tag{$2$}$\n",
    "\n",
    "* among them $v^{'T}_{wj}$ Output matrix $W^{'}$\n",
    "\n",
    "* Finally we calculate the **output of the output layer, the $y_j$ outputas follows:** \n",
    "\n",
    "$y_{c,j} =p(w_{y,j}|w_1,...,w_c) = \\frac{exp(u_{j})}{\\sum^V_{j^{'}=1}exp(u^{'}j)}\\tag{$3$}$\n",
    "\n",
    "## **Learn weights by BP (backpropagation) algorithm and stochastic gradient descent**\n",
    "* Learning weight matrix $W$ versus$W^{'}$In the process, we can assign a **random value to these weights to initialize.** The samples are then **trained in order, and the error between the output and the true value is observed one by one, and the gradient of these errors is calculated.** And correct the **weight matrix** in the gradient direction. This method is called **random gradient descent.** But this derived method is called the back propagation error algorithm. \n",
    "\n",
    "The first is to define the loss function. This loss function is the ***conditional probability of the output word given the input context. It is usually a logarithm, as shown below:** \n",
    "\n",
    "$E = -logp(w_O|w_I)\\tag{$4$}$\n",
    "\n",
    "$= -v_{wo}^T\\cdot h-log\\sum_{j^{'}=1}^Vexp(v^T_w{_{j^{'}}}\\cdot h)\\tag{$5$}$\n",
    "\n",
    "* The next step is to **derive the above probability.** The ***specific derivation process can look at the BP algorithm.*** We get the output weight matrix.$W{‘}$Update rules: \n",
    "\n",
    "$w^{'(new)} = w_{ij}^{'(old)}-\\eta\\cdot(y_{j}-t_{j})\\cdot h_i\\tag{$6$}$\n",
    "\n",
    "* **Equal weight $W{'}$** The update rules are as follows: \n",
    "$w^{(new)} = w_{ij}^{(old)}-\\eta\\cdot\\frac{1}{C}\\cdot EH\\tag{$7$}$\n",
    "\n",
    "### **Psuedo Code**\n",
    "\n",
    "![](https://img-blog.csdn.net/20160719183705872)\n",
    "\n",
    "## **2.2.2.Skip-Gram**\n",
    "\n",
    "---\n",
    "\n",
    "* In many **natural language processing** tasks, many **word expressions** are determined by their **tf-idf** scores. Even though these **scores tell us the relative importance of a word** in a text, they do not tell us the **semantics of the word. Word2vec is a type of neural network model** that, in the case of an **unlabeled corpus, produces a vector that expresses semantics for words in the corpus.** These vectors are usually useful:\n",
    "\n",
    "> * Calculating the semantic similarity of two words by word vector\n",
    "> * Semantic analysis of some supervised NLP tasks such as text categorization\n",
    "\n",
    "* Before go into detail about the **skip-gram model,** let's first understand the format of the **training data.** The input to the **skip-gram model** is a word $w_I$ output is $w_I$  Context ${w_{O,1},...,w_{O,C}}$ the context window size is CC. For example, here is a sentence \"I drive my car to the store.\" If we use \"car\" as the training input data, the word group {\"I\", \"drive\", \"my\", \"to\", \"the\", \"store\"} is the output. For all these words, we will do one-hot coding. The skip-gram model diagram is as follows:\n",
    "\n",
    "![](http://06.imgmini.eastday.com/mobile/20180812/20180812154801_6ec928d180bf51a8d8de0b5c10d8f3f9_2.jpeg)\n",
    "\n",
    "### **Forward propagation**\n",
    "* Next, we look at the skip-gram neural network model. The neural network model of **skip-gram** is improved from the **feedforward neural network** model. It is said that the model is **more effective through some techniques based on the feedforward neural network model.** Let's take a look at the image of a **wave-gram neural network model:** \n",
    "\n",
    "![](http://www.cs.nthu.edu.tw/~shwu/courses/ml/labs/10_Keras_Word2Vec/fig-word2vec-sg.png)\n",
    "\n",
    "\n",
    "* In the above figure, the input **vector $x$ One-hot encoding representing a word,** corresponding ***output vector ${ y1y1,..., yCyC}.$ Weight matrix between input layer and hidden layer $W$The iiRow represents the i in the vocabulary $i$ The weight of the words.*** The next point is : this weight matrix $W's$ the **goal we need to learn (same as $W{‘}$), because this weight matrix contains weight information for all words in the vocabulary.** In the above model, each output **word vector also has a $N× V$ Dimension output vector $W‘$. The final model also has NNThe hidden layer of the node, we can find the hidden layer node hihiThe input is the weighted sum of the input layer inputs. So because of the input vector $x$ one-hot encoding, then only non-zero elements in the vector can produce input to the hidden layer.** So for the input vector **$x$ Where $x_{k^{'}}=0, k\\ne k^{'}$And $xk‘= 0 , k ≠ k‘xk‘=0,k≠k‘$.** So the output of the hidden layer is only with the weight matrix $k$ Row related, mathematically proved as follows: \n",
    "\n",
    "$ h = x^TW=W_{k,.}:=v_{wI}\\tag{$1$} $\n",
    "\n",
    "* Note that since the input is ***one-hot encoded, there is no need to use the activation function here.*** Similarly, **the model output node $C× V$.** The **input** is also **calculated from the weighted sum of the corresponding input nodes:*** \n",
    "\n",
    "$ u_{c,j}=v^{'T}_{wj}h\\tag{$2$} $\n",
    "\n",
    "* In fact, we also saw from the above figure that ***each word in the output layer is shared weight, so we have $u_{c,j}=u_j$*** Finally, we ***generate the C through the softmax function.CThe polynomial distribution of words.*** \n",
    "\n",
    "$ p(w_{c,j}=w_{O,c}|w_{I}) = y_{c,j} = \\frac{exp(u_{c,j})}{\\sum^V_{j^{'}=1}exp(u_{}j^{'})}\\tag{$3$} $\n",
    "\n",
    "* To put it bluntly, this value is the ***probability of the jth node of the $C$ th output word.***\n",
    "\n",
    "## **Learn weights by BP (backpropagation) algorithm and stochastic gradient descent**\n",
    "\n",
    "* the input vector of the ***skip-gram model** and ***the probabilistic expression of the output***, as well as the goals we learned. Next, we explain in detail the ***process of learning weights. The first step is to define the loss function. This loss function is the conditional probability of the output word group. It is usually a logarithm, as shown below:***\n",
    "\n",
    "$E = -logp(w_{O,1},w_{O,2},...,w_{O,C}|w_I)\\tag{$4$}$\n",
    "\n",
    "$= -log\\prod_{c=1}^{C}\\frac{exp(u_{c,j})}{\\sum^V_{j^{'}=1exp(u_j^{'})}}\\tag{$5$}$\n",
    "\n",
    "* The next step is to ***derive the above probability.*** The **specific derivation process** can look at the **BP algorithm**. We get the output **weight matrix.$W‘$Update rules:** \n",
    "\n",
    "$ w^{'(new)} = w_{ij}^{'(old)}-\\eta\\cdot\\sum^{C}_{c=1}(y_{c,j}-t_{c,j})\\cdot h_i\\tag{$6$} $\n",
    "\n",
    "* From the above update rules, we can find that each update requires **summing the entire vocabulary, so for a large corpus, this computational complexity is very high.** So in practical applications, [**Google's Mikolov**](https://arxiv.org/pdf/1310.4546.pdf) et al. proposed that layered softmax and negative sampling can make the computational complexity much lower.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "_uuid": "70602a3fbe922387459e25c39c4056ab128196da"
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize \n",
    "import warnings \n",
    "  \n",
    "warnings.filterwarnings(action = 'ignore') \n",
    "  \n",
    "import gensim \n",
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "_uuid": "d0051b5220ee02c0aeaf1cc3644007b335575c4b"
   },
   "outputs": [],
   "source": [
    "#  Reads ‘alice.txt’ file \n",
    "sample = open(\"../input/text-data/alice.txt\", \"r\") \n",
    "s = sample.read() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "_uuid": "00290122e6f9095bd0391dc05cd9dd737404d02e"
   },
   "outputs": [],
   "source": [
    "# Replaces escape character with space \n",
    "f = s.replace(\"\\n\", \" \") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "_uuid": "31768328685fcced9a8ff1e1c85254a3c448ad53"
   },
   "outputs": [],
   "source": [
    "data = [] \n",
    "  \n",
    "# iterate through each sentence in the file \n",
    "for i in sent_tokenize(f): \n",
    "    temp = [] \n",
    "      \n",
    "    # tokenize the sentence into words \n",
    "    for j in word_tokenize(i): \n",
    "        temp.append(j.lower()) \n",
    "        \n",
    "    data.append(temp) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "c46b04da02be298162f7d2c846ed3096fd631d96"
   },
   "source": [
    "## **CBOW Model**\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "_uuid": "8e93c9293629c4ca6bdf733a5c59523d91d0147d"
   },
   "outputs": [],
   "source": [
    "# Create CBOW model \n",
    "model1 = gensim.models.Word2Vec(data, min_count = 1, size = 100, window = 5) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "_uuid": "a1d6184503181e5dc2e55e22e3920aba4138c8f1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine similarity between 'alice' and 'wonderland' - CBOW :  0.998599\n",
      "Cosine similarity between 'alice' and 'machines' - CBOW :  0.979704\n"
     ]
    }
   ],
   "source": [
    "# Print results \n",
    "print(\"Cosine similarity between 'alice' \" + \"and 'wonderland' - CBOW : \", model1.similarity('alice', 'wonderland')) \n",
    "print(\"Cosine similarity between 'alice' \" +\"and 'machines' - CBOW : \", model1.similarity('alice', 'machines')) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "8bb7de0fd4680746f1a93f85bbd9f04b2095b4da"
   },
   "source": [
    "## **SKIP-GRAM Model**\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "_uuid": "4e0e04cea66f1e1ee01e75bc826cf26856bc0b4b"
   },
   "outputs": [],
   "source": [
    "# Create Skip Gram model \n",
    "model2 = gensim.models.Word2Vec(data, min_count = 1, size = 100, window = 5, sg = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "_uuid": "f86cd6ac083fe09b1d72b879452964252c8f75bb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine similarity between 'alice' and 'wonderland' - Skip Gram :  0.94563484\n",
      "Cosine similarity between 'alice' and 'machines' - Skip Gram :  0.9324432\n"
     ]
    }
   ],
   "source": [
    "# Print results \n",
    "print(\"Cosine similarity between 'alice' \" + \"and 'wonderland' - Skip Gram : \", model2.similarity('alice', 'wonderland')) \n",
    "print(\"Cosine similarity between 'alice' \" + \"and 'machines' - Skip Gram : \", model2.similarity('alice', 'machines'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "ce93e938cbd8f7bf06011109c623afc3fb393095"
   },
   "source": [
    "## **3.Using pre-trained word vectors**\n",
    "\n",
    "---\n",
    "\n",
    "## **1.Glove**\n",
    "- **Official Page : https://nlp.stanford.edu/projects/glove/  (Reading More About Glove)**\n",
    "- **Original Paper: https://nlp.stanford.edu/pubs/glove.pdf**\n",
    " \n",
    "### **Installation of Glove-python**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting glove_python\r\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3e/79/7e7e548dd9dcb741935d031117f4bed133276c2a047aadad42f1552d1771/glove_python-0.1.0.tar.gz (263kB)\r\n",
      "\u001b[K    100% |████████████████████████████████| 266kB 8.9MB/s \r\n",
      "\u001b[?25hRequirement already satisfied: numpy in /opt/conda/lib/python3.6/site-packages (from glove_python) (1.16.0)\r\n",
      "Requirement already satisfied: scipy in /opt/conda/lib/python3.6/site-packages (from glove_python) (1.1.0)\r\n",
      "Building wheels for collected packages: glove-python\r\n",
      "  Running setup.py bdist_wheel for glove-python ... \u001b[?25l-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \bdone\r\n",
      "\u001b[?25h  Stored in directory: /tmp/.cache/pip/wheels/88/4b/6d/10c0d2ad32c9d9d68beec9694a6f0b6e83ab1662a90a089a4b\r\n",
      "Successfully built glove-python\r\n",
      "Installing collected packages: glove-python\r\n",
      "Successfully installed glove-python-0.1.0\r\n",
      "\u001b[33mYou are using pip version 18.1, however version 19.0.3 is available.\r\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\r\n"
     ]
    }
   ],
   "source": [
    "# https://medium.com/data-science-group-iitr/word-embedding-2d05d270b285\n",
    "!pip install glove_python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "_uuid": "42e78d2a481323b6565a9938e1c264e1c015f888"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "\n",
    "from glove import Corpus, Glove\n",
    "from nltk.corpus import gutenberg\n",
    "from multiprocessing import Pool\n",
    "from scipy import spatial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "ec6ce8cadb926185f399ded4351916ece10e9ae1"
   },
   "source": [
    "## **Import training dataset**\n",
    "* Import Shakespeare's Hamlet corpus from nltk library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "_uuid": "5ecfb13ebed0fa455b633b6e851170e01d123e70"
   },
   "outputs": [],
   "source": [
    "sentences = list(gutenberg.sents('shakespeare-hamlet.txt')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "_uuid": "32863005e3f8514973cdd79f49a28cd248557b1b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[', 'The', 'Tragedie', 'of', 'Hamlet', 'by', 'William', 'Shakespeare', '1599', ']']\n",
      "['Actus', 'Primus', '.']\n",
      "['Fran', '.']\n"
     ]
    }
   ],
   "source": [
    "print(sentences[0])    # title, author, and year\n",
    "print(sentences[1])\n",
    "print(sentences[10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "9d22d42789e265b7799353b63fcb3376a8cca503"
   },
   "source": [
    "## **Preprocess data**\n",
    "* Use re module to preprocess data\n",
    "* Convert all letters into lowercase\n",
    "* Remove punctuations, numbers, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "_uuid": "7cb9a1418eb5886dc3faa8a7fd1fdcb437513190"
   },
   "outputs": [],
   "source": [
    "for i in range(len(sentences)):\n",
    "    sentences[i] = [word.lower() for word in sentences[i] if re.match('^[a-zA-Z]+', word)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "_uuid": "d4c537402e38f7e1cc2c22043429bd285f12b24f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', 'tragedie', 'of', 'hamlet', 'by', 'william', 'shakespeare']\n",
      "['actus', 'primus']\n",
      "['fran']\n"
     ]
    }
   ],
   "source": [
    "print(sentences[0])    # title, author, and year\n",
    "print(sentences[1])\n",
    "print(sentences[10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "93d78f54ac931f5db5e6ef1019c1085358bb47d5"
   },
   "source": [
    "## **Create Corpus instance**\n",
    "* Sentences should be fitted into the Corpus instance\n",
    "* Recall that GloVe takes advantage of both count-based matrix factorization and local context-based window methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "_uuid": "0901028f390e2f9dcce2e79934e6275f5a3be657"
   },
   "outputs": [],
   "source": [
    "corpus = Corpus()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "_uuid": "420bf528ed7737f7371904d28b30f2b0d7d0fca4"
   },
   "outputs": [],
   "source": [
    "corpus.fit(sentences, window = 3)    # window parameter denotes the distance of context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "_uuid": "f4a0c8440edc6f8474253e1f120a82c3f24a83e5"
   },
   "outputs": [],
   "source": [
    "glove = Glove(no_components = 100, learning_rate = 0.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "a6f3b8d077542b1d0a8f75545c9157f94f4cc140"
   },
   "source": [
    "## **Train model**\n",
    "* GloVe model is trained with corpus matrix (global statistics of words)\n",
    "* Key parameter description\n",
    "    * **matrix**: co-occurence matrix of the corpus\n",
    "    * **epochs**: number of epochs (i.e., training iterations)\n",
    "    * **no_threads**: number of training threads\n",
    "    * **verbose**: whether to print out the progress messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "_kg_hide-output": true,
    "_uuid": "26500641a71d4e29af91cd28fbb6205e6abf5da4"
   },
   "outputs": [],
   "source": [
    "glove.fit(matrix = corpus.matrix, epochs = 40, no_threads = Pool()._processes, verbose = False)\n",
    "glove.add_dictionary(corpus.dictionary)    #  supply a word-id dictionary to allow similarity queries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "0977f183ab82d5cc0999b7963ee0b7f7a996a7f9"
   },
   "source": [
    "## **Save and load model**\n",
    "* word2vec model can be saved and loaded locally\n",
    "* Doing so can reduce time to train model again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "_uuid": "bd7c387bbbeca4c78167e4c05b731fcfc1290da2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<glove.glove.Glove at 0x7f670189fe48>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glove.save('glove_model')\n",
    "glove.load('glove_model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "7fe8e26fdbfeda652e346069558de29164598453"
   },
   "source": [
    "## **Similarity calculation**\n",
    "* Similarity between embedded words (i.e., vectors) can be computed using metrics such as cosine similarity\n",
    "* For other metrics and comparisons between them, refer to: https://github.com/taki0112/Vector_Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "_uuid": "a38b7eb4f4bc7b62d00ac6a6c0c52ae5a582107f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('queene', 0.9944288763649561),\n",
       " ('matter', 0.9923416888286443),\n",
       " ('rest', 0.9905364974830612),\n",
       " ('against', 0.9881272843339302),\n",
       " ('players', 0.9877037264232148),\n",
       " ('winde', 0.9854944445347792),\n",
       " ('first', 0.9794307052998981),\n",
       " ('drinke', 0.9774403916734686),\n",
       " ('world', 0.9760692025030918)]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glove.most_similar('king', number = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "_uuid": "d7a81ef854c711f73c921a4763aa26a8c4a27b11"
   },
   "outputs": [],
   "source": [
    "# define a function that converts word into embedded vector\n",
    "def vector_converter(word):\n",
    "    idx = glove.dictionary[word]\n",
    "    return glove.word_vectors[idx]\n",
    "\n",
    "\n",
    "# define a function that computes cosine similarity between two words\n",
    "def cosine_similarity(v1, v2):\n",
    "    return 1 - spatial.distance.cosine(v1, v2)\n",
    "\n",
    "\n",
    "v1 = vector_converter('king')\n",
    "v2 = vector_converter('queen')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "_uuid": "019f8e27a1191ed078e8dbb93f6fd57024e88638"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5253676095555772"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_similarity(v1, v2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "e673d9bba1177405e7eb7e0143959253aa66f578"
   },
   "source": [
    "---\n",
    "\n",
    "## **2.Sentence modeling**\n",
    "\n",
    "---\n",
    "\n",
    "Reference Paper : https://arxiv.org/abs/1704.05358\n",
    "* One of the methods to represent sentences as vectors (Mu et al 2017)\n",
    "* Computing vector representations of each embedded word, and weight average them using PCA\n",
    "* If there are n words in a sentence, select N words with high explained variance (n>N)\n",
    "* Most of \"energy\" (around 80%) can be containted using only 4 words (N=4) in the original paper (Mu et al 2017)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "_uuid": "67f313f5f0dcd214760b5cbde3b7e6604a34ccfd"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "from nltk.corpus import gutenberg\n",
    "from multiprocessing import Pool\n",
    "from scipy import spatial\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "_uuid": "097c82e04477517edadcd5523604496189235910"
   },
   "outputs": [],
   "source": [
    "sentences = list(gutenberg.sents('shakespeare-hamlet.txt'))   # import the corpus and convert into a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "_uuid": "9cc8c7aefd56f38c3bf2f43b57abd0f12facd03b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type of corpus:  <class 'list'>\n",
      "Length of corpus:  3106\n"
     ]
    }
   ],
   "source": [
    "print('Type of corpus: ', type(sentences))\n",
    "print('Length of corpus: ', len(sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "_uuid": "56f664e92bd42cf7b9444b56f7749f6fa424bfe1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[', 'The', 'Tragedie', 'of', 'Hamlet', 'by', 'William', 'Shakespeare', '1599', ']']\n",
      "['Actus', 'Primus', '.']\n",
      "['Fran', '.']\n"
     ]
    }
   ],
   "source": [
    "print(sentences[0])    # title, author, and year\n",
    "print(sentences[1])\n",
    "print(sentences[10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "_uuid": "fb963f341217cbf550da75736f79f1fca0238cd9"
   },
   "outputs": [],
   "source": [
    "for i in range(len(sentences)):\n",
    "    sentences[i] = [word.lower() for word in sentences[i] if re.match('^[a-zA-Z]+', word)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "_uuid": "87362bf7f7095b9663316f506608481ca979b27c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', 'tragedie', 'of', 'hamlet', 'by', 'william', 'shakespeare']\n",
      "['actus', 'primus']\n",
      "['fran']\n"
     ]
    }
   ],
   "source": [
    "print(sentences[0])    # title, author, and year\n",
    "print(sentences[1])\n",
    "print(sentences[10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "_uuid": "9c7ca38d2266bd89ad38422c024d84c74c072c51"
   },
   "outputs": [],
   "source": [
    "# set threshold to consider only sentences longer than certain integer\n",
    "threshold = 5\n",
    "\n",
    "for i in range(len(sentences)):\n",
    "    if len(sentences[i]) < 5:\n",
    "        sentences[i] = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "_uuid": "eb12270a363d895541a9b8a12b0bf2cc04d14f8c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of corpus:  1442\n"
     ]
    }
   ],
   "source": [
    "sentences = [sentence for sentence in sentences if sentence is not None]\n",
    "print('Length of corpus: ', len(sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "_uuid": "609c5a6ea206d6a0e56379d22fa29765bd03c430"
   },
   "outputs": [],
   "source": [
    "model = Word2Vec(sentences = sentences, size = 100, sg = 1, window = 3, min_count = 1, iter = 10, workers = Pool()._processes)\n",
    "model.init_sims(replace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "_uuid": "64864fe9b1c8d7db044fb82ed8086d7db27942a4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([ 0.08335786, -0.07213513, -0.20063809,  0.05081655,  0.09836033,\n",
      "        0.15220012, -0.05886959, -0.01153508,  0.09016863,  0.01645453,\n",
      "        0.12024643,  0.05838461, -0.13654494, -0.10049982, -0.1354872 ,\n",
      "        0.15461814,  0.20663305,  0.15638334, -0.04274949, -0.0652777 ,\n",
      "       -0.02395124,  0.11025704,  0.04690168,  0.08275189, -0.03971412,\n",
      "        0.22879153,  0.15806663,  0.01348209,  0.04859876, -0.12926978,\n",
      "        0.0583923 ,  0.00779952,  0.03206569,  0.00270079, -0.01983257,\n",
      "       -0.05165885,  0.02987897,  0.02768921,  0.18233198, -0.06439023,\n",
      "        0.00360876,  0.08085674,  0.13507879,  0.1953338 ,  0.06515917,\n",
      "        0.13446087,  0.06147116, -0.15422224,  0.07960836, -0.00654449,\n",
      "       -0.17003214,  0.13114767, -0.03374168, -0.04970448, -0.05062852,\n",
      "        0.19883926, -0.03758239, -0.02747432,  0.02194867,  0.05582333,\n",
      "        0.0244682 ,  0.04731939,  0.04571397, -0.05661085,  0.03334177,\n",
      "       -0.09297622,  0.00407487,  0.08623616, -0.0033942 , -0.17512482,\n",
      "        0.00630364, -0.07975464,  0.22110683,  0.04305914,  0.05238546,\n",
      "        0.09138442, -0.05016588, -0.09241048,  0.02380024,  0.04419094,\n",
      "        0.07436642,  0.14191374,  0.01328834,  0.06650794,  0.04497926,\n",
      "        0.06109706,  0.20374365, -0.05035355, -0.04018119, -0.09140553,\n",
      "       -0.07272813, -0.09579135, -0.04277504, -0.01752609, -0.17423968,\n",
      "       -0.14214438, -0.23619431, -0.03126049,  0.02792946, -0.04580053],\n",
      "      dtype=float32), array([ 0.08498973, -0.04173244, -0.19439527,  0.04602893,  0.10095516,\n",
      "        0.15134834, -0.06564546, -0.01191569,  0.08886667,  0.0178024 ,\n",
      "        0.12011951,  0.04161036, -0.11994971, -0.08615766, -0.14137903,\n",
      "        0.13757928,  0.21488254,  0.14867868, -0.03368568, -0.07636424,\n",
      "       -0.01013662,  0.10793952,  0.03265708,  0.10044245, -0.03424706,\n",
      "        0.22087887,  0.15759096, -0.00274536,  0.0399179 , -0.15037769,\n",
      "        0.04104736, -0.00218225,  0.03254087,  0.02284817, -0.0207122 ,\n",
      "       -0.06130181,  0.02845738,  0.02627751,  0.19659165, -0.07127944,\n",
      "       -0.01588693,  0.08795823,  0.13946627,  0.18921085,  0.07637312,\n",
      "        0.1373008 ,  0.05708896, -0.13792492,  0.07678931, -0.00074431,\n",
      "       -0.16910008,  0.1398528 , -0.03803712, -0.04051173, -0.05301206,\n",
      "        0.19534218, -0.05229907, -0.01596541,  0.04908691,  0.04564906,\n",
      "        0.02072365,  0.04138613,  0.02998958, -0.05832667,  0.02175297,\n",
      "       -0.10208623,  0.0008705 ,  0.0957182 ,  0.00297812, -0.19213414,\n",
      "        0.01962531, -0.06986419,  0.22626776,  0.03281918,  0.06491695,\n",
      "        0.10498086, -0.06186188, -0.08486271,  0.04270941,  0.05308281,\n",
      "        0.09558274,  0.16535962,  0.00198352,  0.07265703,  0.04598406,\n",
      "        0.0666152 ,  0.17547834, -0.03554152, -0.02528791, -0.10553971,\n",
      "       -0.0755055 , -0.09436686, -0.04398939, -0.02872258, -0.15738025,\n",
      "       -0.13824874, -0.22934663, -0.03774172,  0.00284596, -0.05937549],\n",
      "      dtype=float32), array([ 0.08854863, -0.04025139, -0.20095764,  0.03877187,  0.09401379,\n",
      "        0.15120032, -0.07600588, -0.01518793,  0.09399975,  0.00884566,\n",
      "        0.13944706,  0.05346882, -0.12438133, -0.08208206, -0.13409175,\n",
      "        0.14247398,  0.20571312,  0.15720639, -0.03301229, -0.06629334,\n",
      "       -0.01406477,  0.09804989,  0.04669797,  0.08180197, -0.04497128,\n",
      "        0.23212181,  0.16406424, -0.00120123,  0.05349334, -0.13282977,\n",
      "        0.03109415, -0.01477747,  0.02646029,  0.00929526, -0.02106124,\n",
      "       -0.06357206,  0.02813215,  0.01931615,  0.18169321, -0.06225735,\n",
      "       -0.02007125,  0.09079418,  0.14003904,  0.19414648,  0.0839378 ,\n",
      "        0.14517799,  0.05940082, -0.14030007,  0.08187534, -0.0060426 ,\n",
      "       -0.17973727,  0.1332572 , -0.04256032, -0.03283431, -0.04255338,\n",
      "        0.19680889, -0.05229121, -0.02748378,  0.03291347,  0.04860346,\n",
      "        0.01331992,  0.05564349,  0.02693942, -0.05858523,  0.02769163,\n",
      "       -0.09533632, -0.00595877,  0.08366074, -0.00959623, -0.20000382,\n",
      "        0.00462843, -0.08605112,  0.22428961,  0.04032539,  0.06450354,\n",
      "        0.10376547, -0.06037148, -0.08495244,  0.04457294,  0.0484805 ,\n",
      "        0.06502941,  0.15298821,  0.01359492,  0.07559685,  0.04388659,\n",
      "        0.05554783,  0.18590511, -0.03648281, -0.03812944, -0.0949258 ,\n",
      "       -0.06985681, -0.08318903, -0.04495282, -0.00976487, -0.15973115,\n",
      "       -0.1331825 , -0.23592211, -0.04080522,  0.01561046, -0.04695892],\n",
      "      dtype=float32), array([ 7.67456517e-02, -4.24366891e-02, -2.07015857e-01,  5.14561795e-02,\n",
      "        1.03193723e-01,  1.49754450e-01, -7.49947727e-02, -2.20403504e-02,\n",
      "        7.85547495e-02,  7.96558522e-03,  1.20814957e-01,  2.34400723e-02,\n",
      "       -9.66244712e-02, -8.34066197e-02, -1.51455104e-01,  1.43753886e-01,\n",
      "        2.25242853e-01,  1.33784860e-01, -3.22418883e-02, -7.02833235e-02,\n",
      "       -8.96234997e-03,  9.68973488e-02,  2.39114761e-02,  1.01828218e-01,\n",
      "       -4.23782319e-02,  2.17369989e-01,  1.50919855e-01, -4.60437592e-03,\n",
      "        3.74233164e-02, -1.37252450e-01,  4.32165675e-02,  4.52969194e-04,\n",
      "        3.24156955e-02,  2.11126897e-02, -1.75338276e-02, -6.39963225e-02,\n",
      "        2.15100199e-02,  3.81216742e-02,  1.94882303e-01, -7.02708960e-02,\n",
      "       -1.42927822e-02,  1.00024559e-01,  1.40867636e-01,  1.82408959e-01,\n",
      "        8.98429602e-02,  1.38256982e-01,  6.17303029e-02, -1.39039263e-01,\n",
      "        5.78259453e-02, -1.84156830e-04, -1.70562908e-01,  1.51609108e-01,\n",
      "       -3.79890911e-02, -5.01122475e-02, -5.15428744e-02,  2.08986014e-01,\n",
      "       -5.26991785e-02, -1.81063563e-02,  5.80009073e-02,  4.28208113e-02,\n",
      "        1.98753998e-02,  4.12565358e-02,  3.65492888e-02, -6.04720674e-02,\n",
      "        2.15642992e-02, -1.04153372e-01,  7.90309533e-03,  9.13844183e-02,\n",
      "       -3.94658186e-03, -1.89697668e-01,  1.69733856e-02, -6.60878420e-02,\n",
      "        2.33427450e-01,  3.40703204e-02,  6.32743239e-02,  1.07186064e-01,\n",
      "       -4.41869274e-02, -7.34120011e-02,  5.39136566e-02,  6.16538189e-02,\n",
      "        9.23172459e-02,  1.61940411e-01,  1.02184375e-03,  7.15198070e-02,\n",
      "        4.44311984e-02,  5.91756739e-02,  1.71105802e-01, -4.47191000e-02,\n",
      "       -2.49247216e-02, -1.22598827e-01, -8.01833048e-02, -9.95507613e-02,\n",
      "       -5.04640676e-02, -3.50678191e-02, -1.55704543e-01, -1.24721393e-01,\n",
      "       -2.21743196e-01, -3.37843113e-02,  2.17977632e-03, -6.17262982e-02],\n",
      "      dtype=float32), array([ 0.07921136, -0.04331454, -0.20346238,  0.04989701,  0.10006297,\n",
      "        0.1585245 , -0.06921525, -0.02150072,  0.07848869,  0.00594366,\n",
      "        0.12948468,  0.04043861, -0.10708693, -0.08059696, -0.14597261,\n",
      "        0.14778374,  0.21969406,  0.13744788, -0.02717525, -0.07331597,\n",
      "       -0.0165259 ,  0.09568413,  0.03822304,  0.08607998, -0.03718932,\n",
      "        0.21522143,  0.15635003, -0.00547318,  0.03609021, -0.13761835,\n",
      "        0.04165858, -0.005817  ,  0.02511415,  0.01569063, -0.02287596,\n",
      "       -0.06774794,  0.01914199,  0.02200047,  0.19624586, -0.07139475,\n",
      "       -0.02087856,  0.09422251,  0.14721274,  0.19367918,  0.08377098,\n",
      "        0.14245857,  0.05990143, -0.1405786 ,  0.06994317, -0.00124577,\n",
      "       -0.16408256,  0.13891412, -0.04671497, -0.03512616, -0.049098  ,\n",
      "        0.19694006, -0.05500218, -0.01385589,  0.04824812,  0.03863486,\n",
      "        0.01451261,  0.04773983,  0.0308184 , -0.05575255,  0.01567014,\n",
      "       -0.10256391, -0.00133952,  0.09422773, -0.0142607 , -0.19194472,\n",
      "        0.02057232, -0.06753818,  0.22445804,  0.03294647,  0.06097645,\n",
      "        0.10599516, -0.06126328, -0.08121123,  0.05161159,  0.05567463,\n",
      "        0.09179228,  0.16262384,  0.00352826,  0.07355282,  0.03512207,\n",
      "        0.0595771 ,  0.17860405, -0.04206868, -0.03310748, -0.11401705,\n",
      "       -0.07728773, -0.09629222, -0.04656766, -0.03043797, -0.1564911 ,\n",
      "       -0.13755275, -0.23266576, -0.03140334,  0.00331826, -0.05696482],\n",
      "      dtype=float32), array([ 0.08790252, -0.04022159, -0.20486462,  0.05389725,  0.0909415 ,\n",
      "        0.14469135, -0.05685569, -0.01564073,  0.08315086,  0.02263068,\n",
      "        0.11679234,  0.04635804, -0.10857858, -0.07305235, -0.1436524 ,\n",
      "        0.14696358,  0.21048057,  0.14753962, -0.01877217, -0.08031012,\n",
      "       -0.00538566,  0.11219847,  0.03681328,  0.09814137, -0.04665385,\n",
      "        0.22421245,  0.154837  ,  0.00302809,  0.03757133, -0.15518285,\n",
      "        0.0287711 ,  0.00330148,  0.03224531,  0.01389225, -0.01289656,\n",
      "       -0.05903548,  0.03701548,  0.02367919,  0.20058052, -0.07197198,\n",
      "       -0.01483169,  0.09587196,  0.1471663 ,  0.19440109,  0.07091751,\n",
      "        0.13441893,  0.06869067, -0.13677518,  0.06756873,  0.00039273,\n",
      "       -0.17316246,  0.12496197, -0.04839859, -0.02987431, -0.05038945,\n",
      "        0.19047406, -0.05129518, -0.02720276,  0.03529015,  0.03882585,\n",
      "        0.01556384,  0.03288602,  0.01994853, -0.04469189,  0.0135841 ,\n",
      "       -0.10224736,  0.00761288,  0.08187906,  0.00695009, -0.18724512,\n",
      "        0.02073641, -0.08262794,  0.22941224,  0.04242313,  0.05819914,\n",
      "        0.10755064, -0.05426291, -0.09117705,  0.0437418 ,  0.06450136,\n",
      "        0.08879767,  0.16854444,  0.00091821,  0.06720816,  0.0370649 ,\n",
      "        0.05273232,  0.17620508, -0.03365329, -0.03979865, -0.11164067,\n",
      "       -0.08482701, -0.10162502, -0.03842575, -0.01985638, -0.15491764,\n",
      "       -0.13599765, -0.23680583, -0.03788306, -0.00626481, -0.06700308],\n",
      "      dtype=float32), array([ 0.07859979, -0.0454854 , -0.1896676 ,  0.05808366,  0.09290126,\n",
      "        0.14837678, -0.07058497, -0.00400176,  0.08089922, -0.00072637,\n",
      "        0.13356997,  0.04396908, -0.11710048, -0.09621725, -0.15403579,\n",
      "        0.13425554,  0.22280423,  0.15948664, -0.01312166, -0.05992537,\n",
      "       -0.02353284,  0.12265446,  0.02997149,  0.10526153, -0.03646749,\n",
      "        0.22407164,  0.15261546,  0.00444542,  0.0361768 , -0.14035742,\n",
      "        0.02331944, -0.0026356 ,  0.0356974 ,  0.03411892, -0.00711894,\n",
      "       -0.04977357,  0.01544246,  0.03202773,  0.21092385, -0.06936826,\n",
      "       -0.01473895,  0.08737521,  0.12541908,  0.19344097,  0.08499115,\n",
      "        0.1566479 ,  0.04482907, -0.1294476 ,  0.06952012, -0.01101417,\n",
      "       -0.18012282,  0.12823479, -0.04980932, -0.03031804, -0.05995698,\n",
      "        0.18851699, -0.0685745 , -0.00752864,  0.03817467,  0.06278561,\n",
      "        0.02853539,  0.05285042,  0.01183971, -0.04791077,  0.03409741,\n",
      "       -0.09640984,  0.00420038,  0.09127241, -0.00251596, -0.18270917,\n",
      "        0.02205056, -0.08766726,  0.22659181,  0.02464361,  0.05153127,\n",
      "        0.11504506, -0.0689218 , -0.06878706,  0.02961072,  0.03902859,\n",
      "        0.10614313,  0.15297519,  0.0110751 ,  0.07339806,  0.05686994,\n",
      "        0.06167043,  0.1750878 , -0.05248979, -0.02366859, -0.09836263,\n",
      "       -0.07403374, -0.08908485, -0.05490322, -0.03247558, -0.14071967,\n",
      "       -0.14522998, -0.22714351, -0.01681672,  0.01740743, -0.04175087],\n",
      "      dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "# converting each word into its vector representation\n",
    "for i in range(len(sentences)):\n",
    "    sentences[i] = [model[word] for word in sentences[i]]\n",
    "    \n",
    "print(sentences[0])    # vector representation of first sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "_uuid": "8a0f97248a8859ebc3beeeb371d2b8ee77484dce"
   },
   "outputs": [],
   "source": [
    "# define function to compute weighted vector representation of sentence\n",
    "# parameter 'n' means number of words to be accounted when computing weighted average\n",
    "def sent_PCA(sentence, n = 2):\n",
    "    pca = PCA(n_components = n)\n",
    "    pca.fit(np.array(sentence).transpose())\n",
    "    variance = np.array(pca.explained_variance_ratio_)\n",
    "    words = []\n",
    "    for _ in range(n):\n",
    "        idx = np.argmax(variance)\n",
    "        words.append(np.amax(variance) * sentence[idx])\n",
    "        variance[idx] = 0\n",
    "    return np.sum(words, axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "_uuid": "c00bdf7c394c56ff8ec35f415714d4d23588ddc1"
   },
   "outputs": [],
   "source": [
    "sent_vectorized = []\n",
    "\n",
    "# computing vector representation of each sentence\n",
    "for sentence in sentences:\n",
    "    sent_vectorized.append(sent_PCA(sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "_uuid": "7833a7cf648b2dd590db194d6ed337584cc28fa3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# vector representation of first sentence\n",
    "list(sent_PCA(sentences[0])) == list(sent_vectorized[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "_uuid": "0f32ac14828ba231901c13994f68464ac4182101"
   },
   "outputs": [],
   "source": [
    "# define a function that computes cosine similarity between two words\n",
    "def cosine_similarity(v1, v2):\n",
    "    return 1 - spatial.distance.cosine(v1, v2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "_uuid": "24c721252dceb699f2e6b5977e3fd9ad48f48dd7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9896057844161987\n"
     ]
    }
   ],
   "source": [
    "# similarity between 11th and 101th sentence in the corpus\n",
    "print(cosine_similarity(sent_vectorized[10], sent_vectorized[100]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "e5588fed43cb9af359d395ecaa3b1d1f54f1cc01"
   },
   "source": [
    "---\n",
    "\n",
    "## **3.Doc2Vec**\n",
    "\n",
    "---\n",
    "\n",
    "- Python implementation and application of doc2vec with Gensim\n",
    "- Original paper: Le, Q., & Mikolov, T. (2014). Distributed representations of sentences and documents. In Proceedings of the 31st International Conference on Machine Learning (ICML-14) (pp. 1188-1196)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "_uuid": "74282b14cc68f85a331322faacf5b8c2e9a738a2"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "\n",
    "from gensim.models import Doc2Vec\n",
    "from gensim.models.doc2vec import TaggedDocument\n",
    "from nltk.corpus import gutenberg\n",
    "from multiprocessing import Pool\n",
    "from scipy import spatial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "ea46e7f5b933b53f2d657ba4aebf15fd32b1f2b5"
   },
   "source": [
    "## **Import training dataset**\n",
    "* Import Shakespeare's Hamlet corpus from nltk library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "_uuid": "45419b1fb265b693bf54c6fc7f77729f1c16492e"
   },
   "outputs": [],
   "source": [
    "sentences = list(gutenberg.sents('shakespeare-hamlet.txt'))   # import the corpus and convert into a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "_uuid": "b3cd1aab403f9946a5d37d574c26c28e27e44d2f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type of corpus:  <class 'list'>\n",
      "Length of corpus:  3106\n"
     ]
    }
   ],
   "source": [
    "print('Type of corpus: ', type(sentences))\n",
    "print('Length of corpus: ', len(sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "_uuid": "e07e511ddfd9fd3e1e187650fc71bd9535b81b32"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[', 'The', 'Tragedie', 'of', 'Hamlet', 'by', 'William', 'Shakespeare', '1599', ']']\n",
      "['Actus', 'Primus', '.']\n",
      "['Fran', '.']\n"
     ]
    }
   ],
   "source": [
    "print(sentences[0])    # title, author, and year\n",
    "print(sentences[1])\n",
    "print(sentences[10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "bce7126f98053802476a2e2f77702abdbe128423"
   },
   "source": [
    "## **Preprocess data**\n",
    "* Use re module to preprocess data\n",
    "* Convert all letters into lowercase\n",
    "* Remove punctuations, numbers, etc.\n",
    "* For the doc2vec model, input data should be in format of iterable **TaggedDocuments**\"\n",
    "    * Each TaggedDocument instance comprises **words** and **tags**\n",
    "    * Hence, each document (i.e., a sentence or paragraph) should have a unique tag which is **identifiable**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "_uuid": "2833c34612b2fd6ffe33c4cd7d7f442d67c53884"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', 'tragedie', 'of', 'hamlet', 'by', 'william', 'shakespeare']\n",
      "['actus', 'primus']\n",
      "['fran']\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(sentences)):\n",
    "    sentences[i] = [word.lower() for word in sentences[i] if re.match('^[a-zA-Z]+', word)]\n",
    "    \n",
    "print(sentences[0])    # title, author, and year\n",
    "print(sentences[1])\n",
    "print(sentences[10])\n",
    "\n",
    "for i in range(len(sentences)):\n",
    "    sentences[i] = TaggedDocument(words = sentences[i], tags = ['sent{}'.format(i)])    # converting each sentence into a TaggedDocument"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "_uuid": "c92f9c353157be914a022d4f8d17d12315fb78eb"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TaggedDocument(words=['the', 'tragedie', 'of', 'hamlet', 'by', 'william', 'shakespeare'], tags=['sent0'])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "be00ef273cf0300a84c9e982977d34d90146153a"
   },
   "source": [
    "## **Create and train model**\n",
    "* Create a doc2vec model and train it with Hamlet corpus\n",
    "* Key parameter description (https://radimrehurek.com/gensim/models/doc2vec.html)\n",
    "* **documents**: training data (has to be iterable TaggedDocument instances)\n",
    "    * **size**: dimension of embedding space\n",
    "    * **dm**: DBOW if 0, distributed-memory if 1\n",
    "    * **window**: number of words accounted for each context (if the window size is 3, 3 word in the left neighorhood and 3 word in the right neighborhood are considered)\n",
    "    * **min_count**: minimum count of words to be included in the vocabulary\n",
    "    * **iter**: number of training iterations\n",
    "    * **workers**: number of worker threads to train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "_uuid": "86ccefedba1ac7fbe1248c98fb629a582aca12e6"
   },
   "outputs": [],
   "source": [
    "model = Doc2Vec(documents = sentences, dm = 1, size = 100, window = 3, min_count = 1, iter = 10, workers = Pool()._processes)\n",
    "model.init_sims(replace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "7de34f4a85dc77ef0ba1dd2406d7664334b854de"
   },
   "source": [
    "## **Save and load model**\n",
    "* doc2vec model can be saved and loaded locally\n",
    "* Doing so can reduce time to train model again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "_uuid": "568dbca6168249bcb7d1a7712c332e209a6b5f87"
   },
   "outputs": [],
   "source": [
    "model.save('doc2vec_model')\n",
    "model = Doc2Vec.load('doc2vec_model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "bf225dd8d5e883011790bec25f7c1827fb2604be"
   },
   "source": [
    "## **Similarity calculation**\n",
    "* Similarity between embedded words (i.e., vectors) can be computed using metrics such as cosine similarity\n",
    "* For other metrics and comparisons between them, refer to: https://github.com/taki0112/Vector_Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "_uuid": "c3c83f3936054c6ad2b7f67af26ad029678ac8b9"
   },
   "outputs": [],
   "source": [
    "v1 = model.infer_vector('sent2')    # in doc2vec, infer_vector() function is used to infer the vector embedding of a document\n",
    "v2 = model.infer_vector('sent3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "_uuid": "37717e64b3cd84233ef868740cc07b97c4626341"
   },
   "outputs": [],
   "source": [
    "model.most_similar([v1])\n",
    "# define a function that computes cosine similarity between two words\n",
    "def cosine_similarity(v1, v2):\n",
    "    return 1 - spatial.distance.cosine(v1, v2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "_uuid": "c069487a24a970a691b59e30123d3f63446befb6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9764359593391418"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_similarity(v1, v2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "5f882f609c547f40261b5a8906ac864ca6653bf6"
   },
   "source": [
    "---\n",
    "\n",
    "## **4.Word2Vec**\n",
    "\n",
    "---\n",
    "\n",
    "- Python implementation and application of word2vec with Gensim\n",
    "- Original paper: [Mikolov, T., Chen, K., Corrado, G., & Dean, J. (2013). Efficient estimation of word representations in vector space. arXiv preprint arXiv:1301.3781.](https://arxiv.org/pdf/1301.3781)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "_uuid": "36bca5bd11f763626e3cc8ff76fd0d5ad0bb2810"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "from nltk.corpus import gutenberg\n",
    "from multiprocessing import Pool\n",
    "from scipy import spatial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "4e692b51805ee4e37864471faa7e866110c0610d"
   },
   "source": [
    "## **Import training dataset**\n",
    "- Import Shakespeare's Hamlet corpus from nltk library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "_uuid": "3a9dbfabe4b9c9d9ea50703194e05a348043d67e"
   },
   "outputs": [],
   "source": [
    "sentences = list(gutenberg.sents('shakespeare-hamlet.txt'))   # import the corpus and convert into a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "_uuid": "d6ecdc0b728bf0bc3ea88229d551e1767a6acfe9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type of corpus:  <class 'list'>\n",
      "Length of corpus:  3106\n"
     ]
    }
   ],
   "source": [
    "print('Type of corpus: ', type(sentences))\n",
    "print('Length of corpus: ', len(sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "_uuid": "12b81f86a9d21a0e0e3c697e0433872b786e3abb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[', 'The', 'Tragedie', 'of', 'Hamlet', 'by', 'William', 'Shakespeare', '1599', ']']\n",
      "['Actus', 'Primus', '.']\n",
      "['Fran', '.']\n"
     ]
    }
   ],
   "source": [
    "print(sentences[0])    # title, author, and year\n",
    "print(sentences[1])\n",
    "print(sentences[10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "056dfff7e829cbf36790e050bf055875816d46af"
   },
   "source": [
    "## **Preprocess data**\n",
    "* Use re module to preprocess data\n",
    "* Convert all letters into lowercase\n",
    "* Remove punctuations, numbers, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "_uuid": "5521143a41c4b73b07e0faf3a4f98a68a5eb0323"
   },
   "outputs": [],
   "source": [
    "for i in range(len(sentences)):\n",
    "    sentences[i] = [word.lower() for word in sentences[i] if re.match('^[a-zA-Z]+', word)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "_uuid": "40d0c7e134ddebdd048eaf3fc717784b76c0a05a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', 'tragedie', 'of', 'hamlet', 'by', 'william', 'shakespeare']\n",
      "['actus', 'primus']\n",
      "['fran']\n"
     ]
    }
   ],
   "source": [
    "print(sentences[0])    # title, author, and year\n",
    "print(sentences[1])\n",
    "print(sentences[10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "4ebd64c207bcd50051f4c8d42651eff3fe11e943"
   },
   "source": [
    "## **Create and train model**\n",
    "\n",
    "\n",
    "* Create a word2vec model and train it with Hamlet corpus\n",
    "* Key parameter description (https://radimrehurek.com/gensim/models/word2vec.html)\n",
    "    * sentences: training data (has to be a list with tokenized sentences)\n",
    "    * size: dimension of embedding space\n",
    "    * sg: CBOW if 0, skip-gram if 1\n",
    "    * window: number of words accounted for each context (if the window size is 3, 3 word in the left neighorhood and 3 word in the right neighborhood are considered)\n",
    "    * min_count: minimum count of words to be included in the vocabulary\n",
    "    * iter: number of training iterations\n",
    "    * workers: number of worker threads to train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "_uuid": "d60e4409c23b87d111dfc7a9bf0d867f69ba8b8c"
   },
   "outputs": [],
   "source": [
    "model = Word2Vec(sentences = sentences, size = 100, sg = 1, window = 3, min_count = 1, iter = 10, workers = Pool()._processes)\n",
    "model.init_sims(replace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "294ab00453a216fbc79d29fd38447f4e5fb81266"
   },
   "source": [
    "## **Save and load model**\n",
    "* word2vec model can be saved and loaded locally\n",
    "* Doing so can reduce time to train model again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "_uuid": "4ddadd24344b66e6bb7171780b83f4164d3fd2d8"
   },
   "outputs": [],
   "source": [
    "model.save('word2vec_model')\n",
    "model = Word2Vec.load('word2vec_model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "fdc58d69d37c5a527e252b160f027d0a642b519e"
   },
   "source": [
    "## **Similarity calculation**\n",
    "* Similarity between embedded words (i.e., vectors) can be computed using metrics such as cosine similarity\n",
    "* For other metrics and comparisons between them, refer to: https://github.com/taki0112/Vector_Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "_uuid": "24a57f7ef7b29bca81e3640766f495f1f35781b7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('horatio', 0.9979696869850159),\n",
       " ('king', 0.9978400468826294),\n",
       " ('laertes', 0.997680127620697),\n",
       " ('queene', 0.9975776076316833),\n",
       " ('ophelia', 0.9974517822265625),\n",
       " ('ghost', 0.9973344802856445),\n",
       " ('deere', 0.9972615838050842),\n",
       " ('marcellus', 0.9971654415130615),\n",
       " ('noble', 0.9971342086791992),\n",
       " ('oh', 0.9969217777252197)]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar('hamlet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "_uuid": "504744a10cec2e064c65f1c758c8b15f983b3a48"
   },
   "outputs": [],
   "source": [
    "v1 = model['king']\n",
    "v2 = model['queen']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "_uuid": "af9a285c989760b62bfc997c65944f705d32338e"
   },
   "outputs": [],
   "source": [
    "# define a function that computes cosine similarity between two words\n",
    "def cosine_similarity(v1, v2):\n",
    "    return 1 - spatial.distance.cosine(v1, v2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "_uuid": "ec4e463d7d012a40dc172156c4665f0449c671c9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9965649247169495"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_similarity(v1, v2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "f52f1d7419580ae8b243b440c62e47e6d11f0e09"
   },
   "source": [
    "## **4.Training your own Word Vectors**\n",
    "\n",
    "---\n",
    "\n",
    "* Word2Vec requires that a format of list of list for **training where every document is contained in a list and every list contains list of tokens** of that documents. I won’t be covering the ***pre-preprocessing part here. So let’s take an example list of list to train our word2vec model.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "6efa27ea3555fe0576e2c78b6674f0138d54008e"
   },
   "source": [
    "## **Pretrained Google News Vector Model**\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "_uuid": "e605125d9b6486becc1bb53de9d391500fafe144"
   },
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec, KeyedVectors\n",
    "\n",
    "#loading the downloaded model\n",
    "model = KeyedVectors.load_word2vec_format('../input/word2vec-google/GoogleNews-vectors-negative300.bin', binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "_uuid": "f423ef30e83bcc3372839592ae04722ebfa95ac4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('queen', 0.7118192911148071), ('monarch', 0.6189674139022827), ('princess', 0.5902431011199951), ('crown_prince', 0.5499460697174072), ('prince', 0.5377321243286133), ('kings', 0.5236844420433044), ('Queen_Consort', 0.5235945582389832), ('queens', 0.518113374710083), ('sultan', 0.5098593235015869), ('monarchy', 0.5087411999702454)]\n",
      "cereal\n",
      "0.76640123\n"
     ]
    }
   ],
   "source": [
    "#the model is loaded. It can be used to perform all of the tasks mentioned above.\n",
    "\n",
    "# getting word vectors of a word\n",
    "dog = model['dog']\n",
    "\n",
    "#performing king queen magic\n",
    "print(model.most_similar(positive=['woman', 'king'], negative=['man']))\n",
    "\n",
    "#picking odd one out\n",
    "print(model.doesnt_match(\"breakfast cereal dinner lunch\".split()))\n",
    "\n",
    "#printing similarity index\n",
    "print(model.similarity('woman', 'man'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "c9a9b7a6b873424a7fb2c645c403ad35e95f9467"
   },
   "source": [
    "## **Own Word2Vector Model**\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "_uuid": "5284dea381ad99d1b40c52b703071ebf10c7a5f5"
   },
   "outputs": [],
   "source": [
    "sentence=[['Neeraj','Boy'],['Sarwan','is'],['good','boy']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "_uuid": "8ed03e4b6235e48c3baa52b3e264b55d36fd8299"
   },
   "outputs": [],
   "source": [
    "#training word2vec on 3 sentences\n",
    "model = gensim.models.Word2Vec(sentence, min_count=1,size=300,workers=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "adebf227ba9f3d830cdaa70a93acecf0ab59d7f7"
   },
   "source": [
    "***Let us try to understand the parameters of this model.***\n",
    "\n",
    "* **sentence** – list of list of our corpus\n",
    "* **min_count**=1 -the threshold value for the words. Word with frequency greater than this only are going to be included into the model.\n",
    "* **size**=300 – the number of dimensions in which we wish to represent our word. This is the size of the word vector.\n",
    "* **workers**=4 – used for parallelization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "_uuid": "8f3e10ea9268c2a69254434634ec0ec39bc575b7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.072585374\n"
     ]
    }
   ],
   "source": [
    "#using the model\n",
    "#The new trained model can be used similar to the pre-trained ones.\n",
    "\n",
    "#printing similarity index\n",
    "print(model.similarity('Boy', 'Neeraj'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "62d609ae4b5f8342efa2bb1c3e629fa9a2594a43"
   },
   "source": [
    "### References : \n",
    "1. https://www.analyticsvidhya.com/blog/2017/06/word-embeddings-count-word2veec/\n",
    "2. https://towardsdatascience.com/introduction-to-word-embedding-and-word2vec-652d0c2060fa\n",
    "3. https://www.quora.com/What-is-word-embedding-in-deep-learning\n",
    "4. https://nlp.stanford.edu/projects/glove/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "697b3285804daacd9a53b3a5589b3a88e4b2fc16"
   },
   "source": [
    "# ***Thanks for Reading***"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
